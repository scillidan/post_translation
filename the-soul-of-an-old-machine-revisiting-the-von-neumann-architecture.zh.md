# [旧机器的灵魂：重新审视冯·诺依曼架构](https://ankush.dev/p/neumann_architecture)

_注意：这篇文章基于我本该在一个当地FOSS United聚会上发表的演讲，但由于健康问题未能进行。_

### 序言

> “……这把未来牢牢掌握在那些了解计算机不仅仅是计算机本身，而是它们潜在所有可能性的人手中。”
> 
> “计算机不是目标，它们是我们到达目标的工具。”
> 
> \-- 《暂停与捕捉火焰》，第一季第一集，2014年。

在《暂停与捕捉火焰》中，两位主角之间的对话描述了计算领域中的两类人：

1. 关注于使用计算机解决现实世界问题的人。
2. 关注于解决元问题的人，例如构建计算机。

但这种分类并不是绝对的。

> 当无法进行逻辑思考的人设计大型系统时，这些系统就会变得难以理解。我们开始将其视为生物系统。由于生物系统太复杂而无法理解，因此我们自然而然地认为计算机程序也应如此复杂。
> 
> 我们不应接受这种状况。如果我们接受，那么计算的未来将归属于生物而非逻辑。**我们将继续使用我们无法理解的计算机程序，并试图引导它们做我们想要的事情。**
> 
> **与其拥有一个合理的计算世界，我们将生活在一个顺势疗法和信仰治疗的世界中。**
> 
> \-- 莱斯利·兰波特，《计算的未来：逻辑还是生物》，2003年。

这段引用如果脱离上下文，可能会让人觉得莱斯利·兰波特不尊重生物学家。但这并不是他的本意。他指出，计算机不同于生物学，完全是由我们构建的；只要有足够的动机和能力，人们可以了解计算机内部发生的每一个位翻转。生物学则经历了数百万年的进化，我们无奈地只能通过艰苦的观察自然界来解读。

从事实际问题的工程师需要理解元问题，而解决元问题的工程师需要理解他们解决方案的现实世界影响。没有什么事情是孤立发生的。

现在，撇开这两个看似无关的引述。本文的主要主题是一篇描述现在更普遍称为“冯·诺依曼架构”的论文。这种架构几乎被地球上所有的计算设备所使用，因此从今天的角度来看，他们解决的问题几乎是可以称之为元问题的。

我将主要引用原论文中的值得注意的片段，让论文自行发声。

### 论文

> #### 电子计算仪器的逻辑设计初步讨论
> 
> ##### 阿瑟·W·伯克斯，赫尔曼·H·戈尔丁斯坦，约翰·冯·诺依曼

等等，为什么它叫做“冯·诺依曼”架构，而他在这篇论文中是第三作者？[约翰·冯·诺依曼](https://en.wikipedia.org/wiki/John_von_Neumann)在那个时代是一个举足轻重的人物。在这篇报告发表的前一年，其中一份草稿报告被赫尔曼·戈尔丁斯坦“泄露”了。

这份由冯·诺依曼撰写的泄露草稿本应为内部文件。因此，尽管这项工作本应是许多人在项目中共同努力的结果，但冯·诺依曼的名字却保留了下来。

这次泄露也导致了该项工作无法申请专利。现在很难确定，但这次泄露很可能加速了计算领域的进展一到两个十年。

参考文献：[与J·普雷斯帕·艾克特的口述历史采访](https://conservancy.umn.edu/items/06db4836-2170-4f8f-84cf-1a66c6ff2a1d)

### “位翻转”

新颖的研究可以通过位翻转来理解。“位翻转”是指对现状某一假设的改变（[参考](https://web.stanford.edu/class/cs197/lectures/cs197-02-research-contribution.pdf)）。

例如：

- 现状假定网络行为必须在硬件中静态定义。
- 实际上并不需要如此，我们可以构建软件定义的网络，使网络更加灵活。

因此，在阅读任何开创性的论文时，你肯定会找到一个“位翻转”。假设翻转得越大，论文的影响力就越大。

这篇论文有几个这样的位翻转，但我将专注于其中两个：

> 1.2. 很明显，机器必须能够以某种方式存储不仅在给定计算中所需的数字信息，例如边界值、函数表（例如流体的状态方程），以及计算的中间结果（可能需要不等长度的时间），还必须存储指导实际执行在数值数据上的例程的指令。在专用机器中，这些指令是设备的整体设计结构的一部分。对于**通用机器**，必须可以指示设备执行任何可以用数字形式表述的计算。

参与这个项目的团队之前成功构建了[ENIAC](https://en.wikipedia.org/wiki/ENIAC)。它是为了单一目的而设计的：火炮表计算。他们的下一个机器将成为一台通用计算机。

> 1.3. 从概念上讲，我们在上面讨论了两种不同形式的内存：**数字存储和指令存储**。然而，如果机器的指令被简化为一个数字编码，并且如果机器能够以某种方式区分数字和指令，则内存器可以同时存储数字和指令。

在这篇论文发表之前的计算机有两种不同类型的内存：“指令”和“数字”。随着计算机超越单纯的数值计算机，我们现在称其为“指令”和“数据”。

在可重编程内存中存储程序和数据意味着计算机现在可以在不需要物理重接计算机的情况下重新编程。

### 组件

现在我们来看一下为第一台通用计算机设计的组件描述。

> 1.4. 如果指令的内存仅仅是一个存储器，则必须存在一个可以自动**执行存储在内存中的指令的器官**。我们称这个器官为控制器。

将计算机部件称为“器官”的术语如今已不再流行，但这里的“控制器”描述的是我们现在通常称之为CPU的东西。

> 1.5. 由于该设备是一个计算机器，因此其中必须有一个**算术器官**，能够执行某些基本的算术操作。因此将有一个单元可以进行加、减、乘和除。我们将在下面的6.6中看到，它还可以执行一些经常发生的附加操作。

算术器官就是现代的ALU，处理器内部能够进行算术运算的部分。

> 一般来说，算术单元的内部结构是通过**权衡**运算速度和机器的简单性或廉价性之间的需求来决定的。

他们在构建第一台通用计算机之前就认识到了计算硬件中最大的辩论之一：RISC与CISC。

我们是应该构建简单廉价的组件，需多指令解决问题，还是构建复杂而昂贵的组件，需较少指令？今天看来，我们似乎在CISC样接口与RISC样内部实现之间达成了一致。流行的RISC架构如ARM和CISC架构如x86都有相当复杂的指令集，但复杂指令被划分为较小的["微操作"](https://uops.info/table.html)，这些微操作就是处理器实际执行的内容。

> 总结而言，内存的传输将有两种类型：完全替换，即之前存储的量被清除并替换为一个新数；部分替换，即包含内存位置数字的指令的部分被一个新的内存位置数字替换——我们假设内存中的各个位置按**内存位置编号**串行列举。

在描述对内存的操作时，他们描述的感觉像是指针，但现在我们称之为指针的概念几乎距离前景还有[二十年](https://en.wikipedia.org/wiki/Pointer_(computer_programming)#History)。他们的设计是在指令本身的某个部分对内存位置进行修改，以实现类似“指针”的行为。

> 3.4. 显然，必须能够随时从内存的任意部分获取数字。然而，在指令的情况下，由于可以至少部分地将控制指令按线性顺序排列，因此处理方法可以更有条理。因此，控制器将构建为正常情况下将从内存中的位置n 转移到位置**(n + 1)进行其下一条指令。**

指令不必零散地散布在内存中，可以按顺序加载，因此控制器从一个指令移动到下一个指令。这种行为现在通常与“程序计数器”相关联。

> 3.5. 自动计算机的实用性在于可以一次又一次地使用给定的指令序列，迭代的次数可以是预分配的，也可以依赖于计算的结果。当迭代完成时，必须遵循不同的指令序列，因此我们必须在大多数情况下给出两条平行的指令，前面带有指示要遵循的例程的指令。这个选择可以依赖于数字的符号（机器用途上将零视为正数）。因此，我们引入了一条命令（**条件转移指令**），这条指令将根据给定数字的符号导致执行两个例程中的其中一个。

顺序代码本身无法实现重复的计算。因此，他们提出了跳转指令，根据特定条件可以在控制流中进行跳转。这可以用来实现条件分支和循环。

> 4. 内存器
> 
> 4.1. 理想情况下，希望具有无限大的存储能力，以便可以立即获取任何特定的40 **二进制数字**或字——即在时间上要比一个快速的电子乘法器操作时间短得多。可以假设在约100微秒的水平上是可行的。因此，内存中的一个字的可用时间应该是5到50微秒。同样希望新字的替换速度与此差不多。看来在物理上实现这样的存储能力似乎是不可能的。因此，我们被迫认识到构建**内存层次结构**的可能性，每一种都比前一种具有更大的容量，但访问速度较慢。

这_很奇怪_，他们的机器字大小竟然是“40个二进制数字”。“位”这个术语在那时还没有成为主流，几年前克劳德·香农发表的工作才创造了信息论这一领域。

他们谈论的内存延迟在约100微秒。今天，内存延迟范围从少于1纳秒（L1命中）到100微秒（L3未命中）不等。因此，尽管自诞生以来内存延迟改善了至少三个数量级，但与其他组件速度的提升相比仍然微不足道。这现在被称为“内存墙”。

使用一系列缓存、DRAM和磁盘构建内存层次结构的必要性在通用计算机诞生之时就已经预见到了。然而，这里提到的内存层次结构不太可能是我们今天所知的透明CPU缓存。相反，数据在较快内存与存储之间的移动则由“规划者”或程序员来负责。现代缓存的出现可以追溯到[c. 1960年代](https://en.wikipedia.org/wiki/CPU_cache#History)。

> 电子电路中最常见的存储形式是觸發电路、气体管和电机继电器。为了实现n个字的存储，当然需要大约40n个这样的元素，不包括开关元件。我们早些时候看到，几个千字的快速存储对于一台通用仪器并不难以实现。因此，需要大约 **10^5触发器**或类似元件！显然，这将是**完全不切实际的。**

从近八十年前的评论中读到关于如何10^5个触发器完全不切实际这一点真是有趣。10^5个触发器大约是12.2KiB的内存。你现在使用的浏览器标签可能消耗的内存就比这个多几个数量级。

> 4.5. 鉴于许多高度重要的问题类别需要的总内存远超过2^12字，我们现在考虑存储层级的下一个阶段。尽管偏微分方程的解通常涉及到对成千上万的字的操作，但这些数据通常只在相对较小的块中需要，这些块通常在电子内存的2^12容量范围内。因此，我们的第二种存储形式必须是一个中介，能够将这些字块提供给电子内存。它应该由计算机的控制系统控制，因此是系统的一部分，不需要人为干预。

即使在今天，执行中的程序的“工作集”通常比完整数据或完整程序的大小小得多。通过使用廉价的二级存储，所需的数据可以在需要时加载到内存中，可以满足系统的存储需求。这一概念今天仍然被称为[交换](https://en.wikipedia.org/wiki/Swap_(computer_programming))和[按需分页](https://en.wikipedia.org/wiki/Demand_paging)的组合。

> 媒介应该能够以远低于电子设备的价格记住大量数据。它必须足够快，以便即使在频繁使用时，大部分总解的时间也不会花在将数据读入和读出这种媒介，以及实现所需的位置。如果这个条件没有得到合理满足，**高电子速度的机器优势将大部分失去。**

这一段描述了I/O瓶颈，而我们今天仍在面临这个问题，可能是在不同的方式下。CPU速度显著提高，但内存和磁盘延迟并没有以相同的速度增长。这意味着许多程序的吞吐量受到I/O或内存的限制。

### 数字系统

> 5.2. 在讨论计算机的算术器官时，自然引出了使用何种数字系统的考虑。尽管长久以来在构建数字机器时习惯采用十进制系统，**我们强烈支持为我们的设备采用二进制系统**。我们的基本内存单元自然适应于二进制系统，因为我们不试图测量选择器某一特定点的电荷梯度，而是满足于区分两个状态。触发器本身就是一个真正的二进制设备。在磁性导线或磁带以及声学延迟线存储中，人们也满足于识别脉冲的存在或缺失（如果使用载波频率，则是脉冲列，或脉冲的符号）。

> ...
> 
> 因此，如果考虑使用十进制系统，那么无论是使用图像管还是延迟线存储，就必须将十进制系统进行二进制编码——每个十进制数字必须由至少一个四元组的二进制数字表示。因此，若采用十个十进制数字的精度，就需要至少40个二进制数字。然而，在真正的二进制数字表示中，约33个数字足以实现精度为10^10。因此，二进制系统的使用在设备上更经济。

这解释了“40位字大小”的_奇怪_选择。

他们的前一台机器ENIAC是一个十进制机器。尽管今天二进制计算机似乎随处可见，但在1940年代并非如此。当我们从计算机最小部分的角度来看时，实际上该系统采用二进制是有道理的。

> 5.3. 此外，正在美国和英国建造或计划的几台数字计算机将包含一种所谓的**“浮动小数点”**。这是一种表示每个字的方法，包括特征和尾数——例如，123.45将在机器中表示为(0.12345, 03)，其中3是与该数字关联的10的指数。浮动小数点系统的两个主要目的似乎均源于这样一个事实：字中的数字数量是由特定机器的设计考虑固定的。这两种目的首先是在一个和一个总和中保留尽可能多的有效数字，其次是使人类操作员免于估算并将**“缩放因子”**插入到问题中——这些乘法常数用于保持数字在机器的限制内。

今天我们使用二进制而不是十进制来存储尾数和指数。浮动点数的标准IEEE 754于1985年正式化，FPU直到2000年代才成为PC的主流。虽然这听起来相当奇怪，因为科学计算依赖于“实数”进行计算，而仅用整数进行这些计算是很棘手的。

> 当然，毋庸置疑，人类在为引入适当的缩放因子而消耗时间。我们只辩称，这样消耗的时间在我们为机器准备一个有趣的问题的总时间中占的比例非常小。我们感觉浮动点数的第一个**优点显得有些虚幻**。为了获得这样的浮动点数，就必须浪费可以用来携带更多数字的存储空间。因此，我们认为，对于是否浮动的二进制点的微小优势是否弥补了存储容量的损失和算术及控制电路的复杂性，现在并不清楚。

冯·诺依曼对我们为何需要浮动点数并不完全认同。他对浮动点数的批评表达得很直白。不论是好是坏，浮动点数现在已成为大多数系统中的无处不在的数字类型。

在另一篇论文中，冯·诺依曼对浮动点数的批评甚至更加尖锐：

> “此外，浮动二进制小数点代表了使对部分问题的彻底数学理解不再必要的努力，我们认为这是一个值得怀疑的方向。”

这一论点可能有一定道理。就我个人而言，我花了好几天时间才正确 [四舍五入浮动点数](https://github.com/frappe/frappe/pull/20258)。关于[浮动点数的更多信息在这里](https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html)，你可以仔细阅读。

无关紧要：你是否曾想过为什么Linux系统调用不提供浮动点数作为秒，而是分别提供秒`mtim.tv_sec`和纳秒部分`mtim.tv_nsec`？像Linux这样的操作系统仍然使用整数来通过缩放因子来近似实数，出于各种原因，[浮动点数在内核中的使用被严格禁止](https://docs.kernel.org/core-api/floating-point.html)。

> 然而，这些期望必须与一些进一步的评论结合考虑。具体而言：（a）x和y本身很可能是相似舍入的结果，直接或间接地固有，即x和y本身应视为“真实”的x'和y'值的无偏n位近似；（b）通过谈论“方差”和“均值”，我们引入了统计概念。现在，实际上这些近似与统计性质没有关系，而是由于算法和数字表现的特质（从我们这方面看，是其不足）而实际严谨和唯一地确定的。然而，在当前数学科学的状态下，试图严格处理这些问题似乎是相当**无望的**。

想象一下，20世纪的数学天才在暗示，处理连续舍入操作所造成的舍入错误是无望的。这确实是一个硬问题，想象一下存储多个项目订单的基本场景。

| 产品 | 价格 | 数量 | 小计 | 四舍五入小计 |
| --- | --- | --- | --- | --- |
| 铁 | 1.555 | 7.0 | 10.885 | 10.89 |
| 锌 | 2.555 | 11.0 | 28.105 | 28.11 |
| 总计 |  |  | 38.99 | 39.00 |

如果你必须四舍五入到两位数字，那么你应该在哪里四舍五入这些数字？价格？小计？总计？

- 如果你四舍五入价格，舍入误差会随着数量的增加而增大，因此通常不考虑。
- 四舍五入的小计和总计最多可以相差1美分。这样做有偏见吗？偏向哪一边？

没有“正确”的答案。你只能选择一种方法并坚持下去。这个例子甚至没有考虑浮动点数的复杂性，因为某些浮动点数不能表示某些实数，因此会有额外的表现错误。

如果你认为固定精度数字可能会拯救你，那我有一些消息要告诉你。

```
>>> # 这是 Python REPL
>>> from decimal import Decimal
>>> Decimal("1") / Decimal("3") * Decimal("3") == Decimal("1")
False
```

> 因此，乘法和除法不可避免地必须用两种不同的操作来替代，必须在所有条件下产生n位数字，并且在此限制条件下，应尽可能接近真实的乘法和除法结果。可以称之为伪乘法和伪除法；然而，公认的术语称其为带舍入的乘法和除法。（我们现在创建了一个“加法”和“减法完全自由不受此类缺陷”的印象。这只有在它们不创造新位时才是正确的，因为乘法和除法确实会创造新位。然而，它们可能导致新位的产生，即造成数字“超出范围”的情况。这一复杂性，当然，众所周知，通常会由规划员通过数学安排和估算来应对，以将数字**“控制在范围内”**。

这段话描述了我们现在所称的“溢出”错误。在进行算术操作时尽量保持数字在范围内，可能是今天仍存在的另一个主要错误来源。将这个问题完全留给“规划者”并不总是行得通，人们因为[溢出错误](https://en.wikipedia.org/wiki/Ariane_flight_V88)而失去了一次火箭发射。

### 调试

> 4.8. 输入输出的另一个极其重要的部分，我们仅在此提到，即某种用于**图形化查看给定计算结果的机制**。这当然可以通过选择器类似的管子来实现，当数据通过电子束放到此管子上时，它的屏幕便会发光。

可调试性是复杂系统的一个重要属性。调试不仅是从系统中消除错误的过程，还是你真正理解发生在幕后内容的方式，这可能与您的理解相符，也可能不相符。例如，当我使用`strace`来了解一个程序的工作时，我并不总是寻找错误，有时仅仅是一个了解其运作的过程。

> 当问题第一次运行时，因需要特别小心或者已知存在错误而必须定位的情况下，必须通常有两台机器并行使用。因此，大多数时候它们可以作为独立的机器使用。这种检查方法的一个关键特性在于，它在每个时刻检查计算（因而也检测瞬时错误和稳态错误），并且当出现错误时，**停止机器，以便大大简化定位故障的过程**。

_有趣_。他们使用两台机器检查在每个阶段是否始终产生相同的结果。我们可以找到需要高冗余和可靠性的类似系统，例如核反应堆、[航天计算机](https://en.wikipedia.org/wiki/SpaceX_Merlin#Engine_control)和自主车辆。

其次，他们暂停执行以便规划者检查错误。听起来很熟悉？

> 定位错误的方法，无论是使用还是不使用重复机器，都需要进一步讨论。计划是设计所有电路（包括控制电路），以便如果在脉冲之间停止时钟，则计算机将保留其所有信息在触发器中，因此计算可以在时钟再次启动时不受干扰地继续进行。这一原则在ENIAC中已经证明其实用性。这使得机器能够以低于某一最大值的任何速度进行计算，只要时钟发出的脉冲形状是恒定的，无论脉冲之间的间隔有多大。时钟将提供一种操作模式，在这种模式中，操作员可以根据指示，单独发出一个脉冲。**通过这种方式，操作员可以使机器逐步执行计算步骤**，通过连接到触发器的指示灯来检查结果。

这类似于现代的调试器，允许你逐条指令或逐行代码逐步调试程序。

当编写新版本程序的周期时间很高时，随机尝试假设直至成功不是一种实用的方法。今天，借助现代硬件和快速迭代周期，我们可以在大多数错误情况下不使用调试器。

### 结语 - 八十年后

![冯·诺依曼架构图](https://scillidan.github.io/image_post/the-soul-of-an-old-machine-revisiting-the-von-neumann-architecture_01.webp)

这就是今天学校中教授的冯·诺依曼架构。著名的大O符号也基于类似的计算机模型。然而，在八十年的演变之后，底层硬件与此模型完全不同。

我们的冯·诺依曼架构简易框图具有很大的教育价值，但让我们现实一点，即使冯·诺依曼架构的最初实现也并不简单。让我们来看下现代Zen 4架构的框图。

![Zen 4框图](https://scillidan.github.io/image_post/the-soul-of-an-old-machine-revisiting-the-von-neumann-architecture_02.webp)

图片来源：[Chips and Cheese](https://chipsandcheese.com/p/amds-zen-4-part-1-frontend-and-execution-engine)。

在现代CPU中，_没有_任何事情是简单的。

- 指令不会一次执行一条，数百条指令可以在任何给定时刻同时进行。
- 在每个可行的组件中都有_疯狂_的投机执行：
	- 条件分支通过分支预测器在实际评估之前就被预测。
	- 很多（通常2到8）独立指令可以在单个核心上并行执行。
	- 过时的内存值会被使用，希望它们不会改变，如果它们改变，CPU会回滚错误的计算。
	- 多个预取器在你的代码发出之前发出下一个内存访问指令。
- 内存访问时间从少于1纳秒（L1命中）到300,000纳秒（页面错误+从SSD加载）不等。
- 硬件协助虚拟内存实现，使TLB和分页得以使用。
- 存在可以对多个数据执行单一操作的矢量单元，即SIMD。
- 哦，还有可能有数百个核心，都在尝试呈现系统的一致视图。
- 哦，还有可能在同一板上有超过一个这样的芯片，处于NUMA配置中。

\*mindblown.gif\*

虽然计算机架构师保证你始终获得按规范期望的行为，但他们无法做到同样确保性能。你需要合理地理解所有底层组件，才能真正利用现代硬件的能力，否则我们注定会为[可以用单个笔记本解决的问题而使用计算集群](https://www.usenix.org/system/files/conference/hotos15/hotos15-paper-mcsherry.pdf)。用保罗·巴拉姆的话说：

> 在你证明你能够使用第一台计算机之前，你不能拥有第二台计算机。

---

参考文献/进一步阅读/启发/其他：

- [原始论文](https://www.ias.edu/sites/default/files/library/Prelim_Disc_Logical_Design.pdf)
- [计算机架构：定量方法](https://dl.acm.org/doi/pdf/10.5555/1999263)
- [布莱恩·坎特里尔谈自适应替换缓存](https://www.youtube.com/watch?v=F8sZRBdmqc0)
- [计算的未来：逻辑或生物](https://lamport.azurewebsites.net/pubs/future-of-computing.pdf)
- [暂停与捕捉火焰](https://www.imdb.com/title/tt2543312/)
- [新机器的灵魂](https://www.goodreads.com/book/show/7090.The_Soul_of_a_New_Machine)

_洗澡时的思考：谁会“翻转”冯·诺依曼架构及其八十年的演变？_

我对此主题有点生疏，所以如果你发现这篇文章中的任何拼写或事实错误，请在GitHub上发送更正（页脚中的“源”）或通过电子邮件联系我。

[GPT-4o mini]
